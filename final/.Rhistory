library(tools)
library(quanteda)
library(topicmodels)
library(tidytext)
library(plyr)
# your filepath here
path <- "/Users/madelinebrady/Desktop/Fall 2019/Hertie-ML-TADA-Project/newspaper-data/English"
# create list of all outlets
filenamesList <- list.files(path = path, full.names = TRUE)
# load all .Rdata files
for (i in 1:length(filenamesList)) {
load(filenamesList[i])
}
# creating a character vector of the dataframe names
newsNames <- file_path_sans_ext(basename(filenamesList))
newsNames <- word(newsNames, 1, sep = "_")
newsNames <- paste0(newsNames, "_df")
# load data
breitbart_df$text <- as.character(breitbart_df$text)
foxnews_df$text <- as.character(foxnews_df$text)
nytimes_df$text <- as.character(nytimes_df$text)
thinkprogress_df$text <- as.character(thinkprogress_df$text)
wsj_df$text <- as.character(wsj_df$text)
wsj_df <- wsj_df %>% select(-section, -paywall)
foxnews_df$topic_tags <- "NA"
fullDataSet <- rbind(breitbart_df, foxnews_df, nytimes_df, thinkprogress_df, wsj_df)
# filtering for sports
fullDataSet <- fullDataSet %>%
filter(domain != "Sports" &
domain != "World Cup" &
domain != "Pro Basketball" &
domain != "Baseball" &
domain != "Soccer" &
domain != "Tennis" &
domain != "Pro Football")
# pre-processing
## split the datetime column into two
fullDataSet$time <- format(as.POSIXct(fullDataSet$datetime, format="%Y:%m:%d %H:%M:%S"), "%H:%M:%S")
fullDataSet$date <- format(as.POSIXct(fullDataSet$datetime, format="%Y:%m:%d %H:%M:%S"), "%Y:%m:%d")
fullDataSet <- fullDataSet %>% select(-datetime)
# limiting time period for baseline
fullDataSet <- fullDataSet %>%
filter(date >= "2018:10:26" & date <= "2018:11:12")
# limiting time period for baseline
fullDataSet <- fullDataSet %>%
filter(date >= "2018:10:26" & date <= "2018:11:12")
# create a corpus
baselineCorpus <- corpus(fullDataSet, text_field = "text", metacorpus = NULL, compress = FALSE)
#create a document feature matrix
## tokenize by word, pre-process, create dfm
baselineTokens <- baselineCorpus %>%
tokens(what = "word",
remove_url = TRUE,
remove_punct = TRUE,
remove_separators = TRUE,
remove_numbers = TRUE)
baselineTokens <- tokens_remove(baselineTokens,
stopwords('english'),
min_nchar = 3L)
baselineTokens <- tokens_remove(baselineTokens,
c("said", "like"))
# filtering articles with tokens relating to guns
gunWords <- c("gun", "shooting", "gunman", "shooter",
"guns", "second amendment",
"2nd amendment", "firearm", "firearms",
"NRA", "National Rifle Association")
baselineTokens <- keep(baselineTokens, ~ any(gunWords %in% .x))
# create Dfm
baselineDfm <- dfm(baselineTokens)
# Create an LDA object (without a control argument)
LDA10 <- LDA(baselineDfm, k = 10, control = list(seed = 1234))
LDA15 <- LDA(baselineDfm, k = 15, control = list(seed = 1234))
# Word-topic probabilities analysis of LDA object
## Extract per-topic-per-word probabilities (Beta) using tidytext package
baselineTopics <- tidy(LDA10, matrix = "beta")
### Find the 10 terms that are most common within each topic and vizualize
topicsTopTerms <- baselineTopics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
topicsTopTerms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
index_1 <- sample(1:nrow(fullDataSet), round(nrow(fullDataSet) * 0.7))
train <- fullDataSet[index_1, ]
test  <- fullDataSet[-index_1, ]
# create a corpus
baselineCorpus <- corpus(train, text_field = "text", metacorpus = NULL, compress = FALSE)
#create a document feature matrix
## tokenize by word, pre-process, create dfm
baselineTokens <- baselineCorpus %>%
tokens(what = "word",
remove_url = TRUE,
remove_punct = TRUE,
remove_separators = TRUE,
remove_numbers = TRUE)
baselineTokens <- tokens_remove(baselineTokens,
stopwords('english'),
min_nchar = 3L)
baselineTokens <- tokens_remove(baselineTokens,
c("said", "like"))
# filtering articles with tokens relating to guns
gunWords <- c("gun", "shooting", "gunman", "shooter",
"guns", "second amendment",
"2nd amendment", "firearm", "firearms",
"NRA", "National Rifle Association")
baselineTokens <- keep(baselineTokens, ~ any(gunWords %in% .x))
# create Dfm
baselineDfm <- dfm(baselineTokens)
# Create an LDA object (without a control argument)
LDA10 <- LDA(baselineDfm, k = 10, control = list(seed = 1234))
LDA15 <- LDA(baselineDfm, k = 15, control = list(seed = 1234))
# Word-topic probabilities analysis of LDA object
## Extract per-topic-per-word probabilities (Beta) using tidytext package
baselineTopics <- tidy(LDA10, matrix = "beta")
### Find the 10 terms that are most common within each topic and vizualize
topicsTopTerms <- baselineTopics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
topicsTopTerms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
split LDA into train and test
#split train and test data
set.seed(123)
index_2 <- sample(1:nrow(LDA15), round(nrow(LDA15) * 0.7))
train2 <- fullDataSet[index_2, ]
test2  <- fullDataSet[-index_2, ]
index_2 <- sample(1:nrow(LDA15), round(nrow(LDA15) * 0.7))
install.packages("tuber")
clientId <- "1061480568337-6beoj5kptvsjjnobun87cmkv9aqtah3r.apps.googleusercontent.com"
clientSecret <- "yEDdN3zTYRwVdijve8yMkYEk"
?yt_oauth
library(tuber)
?yt_oauth
library(tuber)
clientId <- "1061480568337-6beoj5kptvsjjnobun87cmkv9aqtah3r.apps.googleusercontent.com"
clientSecret <- "yEDdN3zTYRwVdijve8yMkYEk"
yt_oauth(app_id = clientId, app_secret = clientSecret, token = remove_old_oauth = TRUE)
yt_oauth(app_id = clientId, app_secret = clientSecret, token = '', remove_old_oauth = TRUE)
yt_oauth(app_id = clientId, app_secret = clientSecret, token = '')
library(tuber)
clientId <- "1061480568337-6beoj5kptvsjjnobun87cmkv9aqtah3r.apps.googleusercontent.com"
clientSecret <- "yEDdN3zTYRwVdijve8yMkYEk"
yt_oauth(app_id = clientId, app_secret = clientSecret, token = '')
yt_oauth(app_id = clientId, app_secret = clientSecret, token = '')
# load packages
library(tuber)
# add your client id and client secret
clientId <- "1061480568337-6beoj5kptvsjjnobun87cmkv9aqtah3r.apps.googleusercontent.com"
clientSecret <- "yEDdN3zTYRwVdijve8yMkYEk"
yt_oauth(app_id = clientId, app_secret = clientSecret)
library(dplyr)
# search videos
?yt_search
# search top 5 video including warren and sanders
yt_search(term = "Elizabeth Warren", max_results = 5, type = video, published_after = "2018-31-12T00:00:00Z")
# search top 5 video including warren and sanders
#12-31-2018, 2
yt_search(term = "Elizabeth Warren", max_results = 5, type = video, published_after = "2018-31-12T00:00:00Z")
# search top 5 video including warren and sanders
#12-31-2018, 2-19-2018
yt_search(term = "Elizabeth Warren", max_results = 5, type = video, published_after = "2018-12-31T00:00:00Z")
# search top 5 video including warren and sanders
#12-31-2018, 2-19-2018
yt_search(term = "Elizabeth Warren", max_results = 5, type = 'video', published_after = "2018-12-31T00:00:00Z")
yt_oauth(app_id = clientId, app_secret = clientSecret, token = "")
# search top 5 video including warren and sanders
#12-31-2018, 2-19-2018
yt_search(term = "Elizabeth Warren", max_results = 5, type = 'video', published_after = "2018-12-31T00:00:00Z")
yt_search(term = "Bernie Sanders", max_results = 5, type = 'video', published_after = "2019-19-02T00:00:00Z")
yt_search(term = "Bernie Sanders", max_results = 5, type = 'video', published_after = "2019-02-19T00:00:00Z")
# search top 5 video including warren and sanders
#12-31-2018, 2-19-2018
yt_search(term = "Elizabeth Warren", max_results = 5, published_after = "2018-12-31T00:00:00Z")
# search top 5 video including warren and sanders
#12-31-2018, 2-19-2018
yt_search(term = "Elizabeth Warren", max_results = 5, published_after = "2018-12-31T00:00:00Z")
# search top 5 video including warren and sanders
yt_search(term = "Elizabeth Warren", published_after = "2018-12-31T00:00:00Z")
# get stats on a video
get_all_comments(video_id = "a-UQz7fqR3w")
# get stats on a video
get_stats(video_id = "N708P-A45D0")
# search top 5 video including warren and sanders
yt_search(term = "Elizabeth Warren", max_results = 5, published_after = "2018-12-31T00:00:00Z")
yt_oauth(app_id = clientId, app_secret = clientSecret, token = "")
# search top 5 video including warren and sanders
yt_search(term = "Elizabeth Warren", max_results = 5, published_after = "2018-12-31T00:00:00Z")
# search top 5 video including warren and sanders
yt_search(term = "Barack Obama")
# load packages
library(tuber)
library(dplyr)
# add your client id and client secret
clientId <- "1061480568337-6beoj5kptvsjjnobun87cmkv9aqtah3r.apps.googleusercontent.com"
clientSecret <- "yEDdN3zTYRwVdijve8yMkYEk"
yt_oauth(app_id = clientId, app_secret = clientSecret, token = )
# search top 5 video including warren and sanders
yt_search(term = "Elizabeth Warren", max_results = 5)
# get stats on a video
get_stats(video_id = "N708P-A45D0")
install.packages("vosonSML")
library("vosonSML")
?Authenticate
Authenticate("youtube", clientId)
credential <- yt_oauth(app_id = clientId, app_secret = clientSecret, token = )
Collect(credential = credential)
detach("package:vosonSML", unload=TRUE)
install.packages("jsonlite")
install.packages("jsonlite")
# load packages
library(tuber)
# load packages
install.packages("tuber")
install.packages("tuber")
library(dplyr)
# add your client id and client secret
clientId <- "1061480568337-6beoj5kptvsjjnobun87cmkv9aqtah3r.apps.googleusercontent.com"
clientSecret <- "yEDdN3zTYRwVdijve8yMkYEk"
yt_oauth(app_id = clientId, app_secret = clientSecret, token = "")
library(tuber)
library(dplyr)
# add your client id and client secret
clientId <- "1061480568337-6beoj5kptvsjjnobun87cmkv9aqtah3r.apps.googleusercontent.com"
clientSecret <- "yEDdN3zTYRwVdijve8yMkYEk"
yt_oauth(app_id = clientId, app_secret = clientSecret, token = "")
yt_oauth(app_id = clientId, app_secret = clientSecret, token = '')
# add your client id and client secret
clientId <- "1061480568337-6beoj5kptvsjjnobun87cmkv9aqtah3r.apps.googleusercontent.com"
# load packages
library(tuber)
library(dplyr)
# add your client id and client secret
clientId <- "1061480568337-6beoj5kptvsjjnobun87cmkv9aqtah3r.apps.googleusercontent.com"
clientSecret <- "yEDdN3zTYRwVdijve8yMkYEk"
yt_oauth(app_id = clientId, app_secret = clientSecret, token = '')
### Machine Learning Application of Demographic data on Gun Violence in U.S. ####
# load packages
install.packages("caret")
install.packages("ranger")
install.packages("h2o")
install.packages("rsample")
install.packages("dplyr")    # alternative installation of the %>%
library(caret)
library(dplyr)    # for data wrangling
library(ggplot2)  # for awesome graphics
library(rsample)
library(ranger)   # for a fast c++ implementation of random forest
library(h2o)
library(dplyr)
library(tidyverse)
install.packages("randomForest")
library(randomForest)
# h2o set-up
h2o.no_progress()  # turn off h2o progress bars
h2o.init()         # launch h2o
# your filepath here
path <- "/Users/AEMagaard/Documents/Hertie/Semester 3/Text Analysis/Project/Hertie-ML-TADA-Project-master"
# adding independent variables to data set
fullDataSet <- read.csv("preppedDataSet.csv")
demographics <- read.csv("demoData.csv")
fullDataSet <- fullDataSet %>%
mutate(date = as.Date(date),
topic = as.factor(topic))
demographics <- demographics %>%
mutate(date = as.Date(date),
children = as.factor(ifelse(children == "Yes", 1, 0)),
murderSuicide = as.factor(ifelse(murderSuicide == "Yes", 1, 0)),
countyVote = as.factor(ifelse(countyVote == "Yes", 1, 0)),
majorityRace = as.factor(case_when(
majorityRace == "White" ~ 1,
majorityRace == "Black" ~ 2,
majorityRace == "Latino" ~ 3,
majorityRace == "Asian" ~ 4,
majorityRace == "American Indian" ~ 5)),
medianIncome = as.numeric(medianIncome),
medianIncome = log(medianIncome),
zip = as.factor(zip)
)
joinedDataSet <- left_join(fullDataSet, demographics) %>%
fill(c(6:15), .direction = c("down"))
str(joinedDataSet)
dim(joinedDataSet)
#[1] 2937   15
#split data into training and test
#using simple random sampling using caret package
set.seed(123)  # for reproducibility
index_2 <- createDataPartition(joinedDataSet$topic, p = 0.7,
list = FALSE)
train <- joinedDataSet[index_2, ]
test  <- joinedDataSet[-index_2, ]
### CODE BLOCK: Following code for Random Forest model ###
## first default random forest model using the caret package
random_forest <- train(topic ~ .,
data = train,
method = "ranger")
random_forest
library(caret)
library(dplyr)    # for data wrangling
library(ggplot2)  # for awesome graphics
library(rsample)
library(ranger)   # for a fast c++ implementation of random forest
library(h2o)
library(dplyr)
library(tidyverse)
install.packages("randomForest")
library(randomForest)
# h2o set-up
h2o.no_progress()  # turn off h2o progress bars
h2o.init()         # launch h2o
install.packages("randomForest")
library(caret)
library(dplyr)    # for data wrangling
library(ggplot2)  # for awesome graphics
library(rsample)
library(ranger)   # for a fast c++ implementation of random forest
library(h2o)
library(dplyr)
library(tidyverse)
library(randomForest)
# h2o set-up
h2o.no_progress()  # turn off h2o progress bars
h2o.init()         # launch h2o
# your filepath here
path <- "/Users/madelinebrady/Documents/GitHub/Hertie-ML-TADA-Project"
# adding independent variables to data set
fullDataSet <- read.csv("preppedDataSet.csv")
demographics <- read.csv("demoData.csv")
fullDataSet <- fullDataSet %>%
mutate(date = as.Date(date),
topic = as.factor(topic))
demographics <- demographics %>%
mutate(date = as.Date(date),
children = as.factor(ifelse(children == "Yes", 1, 0)),
murderSuicide = as.factor(ifelse(murderSuicide == "Yes", 1, 0)),
countyVote = as.factor(ifelse(countyVote == "Yes", 1, 0)),
majorityRace = as.factor(case_when(
majorityRace == "White" ~ 1,
majorityRace == "Black" ~ 2,
majorityRace == "Latino" ~ 3,
majorityRace == "Asian" ~ 4,
majorityRace == "American Indian" ~ 5)),
medianIncome = as.numeric(medianIncome),
medianIncome = log(medianIncome),
zip = as.factor(zip)
)
joinedDataSet <- left_join(fullDataSet, demographics) %>%
fill(c(6:15), .direction = c("down"))
str(joinedDataSet)
dim(joinedDataSet)
#[1] 2937   15
#split data into training and test
#using simple random sampling using caret package
set.seed(123)  # for reproducibility
index_2 <- createDataPartition(joinedDataSet$topic, p = 0.7,
list = FALSE)
train <- joinedDataSet[index_2, ]
test  <- joinedDataSet[-index_2, ]
### CODE BLOCK: Following code for Random Forest model ###
## first default random forest model using the caret package
random_forest <- train(topic ~ .,
data = train,
method = "ranger")
random_forest
# your filepath here
path <- "/Users/madelinebrady/Documents/GitHub/Hertie-ML-TADA-Project"
# adding independent variables to data set
fullDataSet <- read.csv("preppedDataSet.csv")
demographics <- read.csv("demoData.csv")
fullDataSet <- fullDataSet %>%
mutate(date = as.Date(date),
topic = as.factor(topic))
demographics <- demographics %>%
mutate(date = as.Date(date),
children = as.factor(ifelse(children == "Yes", 1, 0)),
murderSuicide = as.factor(ifelse(murderSuicide == "Yes", 1, 0)),
countyVote = as.factor(ifelse(countyVote == "Yes", 1, 0)),
majorityRace = as.factor(case_when(
majorityRace == "White" ~ 1,
majorityRace == "Black" ~ 2,
majorityRace == "Latino" ~ 3,
majorityRace == "Asian" ~ 4,
majorityRace == "American Indian" ~ 5)),
medianIncome = as.numeric(medianIncome),
medianIncome = log(medianIncome),
zip = as.factor(zip)
)
joinedDataSet <- left_join(fullDataSet, demographics) %>%
fill(c(6:15), .direction = c("down"))
library(caret)
library(dplyr)    # for data wrangling
library(ggplot2)  # for awesome graphics
library(rsample)
library(ranger)   # for a fast c++ implementation of random forest
library(h2o)
library(dplyr)
library(tidyverse)
library(randomForest)
# h2o set-up
h2o.no_progress()  # turn off h2o progress bars
h2o.init()         # launch h2o
# your filepath here
path <- "/Users/madelinebrady/Documents/GitHub/Hertie-ML-TADA-Project/final"
# adding independent variables to data set
fullDataSet <- read.csv("preppedDataSet.csv")
demographics <- read.csv("demoData.csv")
fullDataSet <- fullDataSet %>%
mutate(date = as.Date(date),
topic = as.factor(topic))
demographics <- demographics %>%
mutate(date = as.Date(date),
children = as.factor(ifelse(children == "Yes", 1, 0)),
murderSuicide = as.factor(ifelse(murderSuicide == "Yes", 1, 0)),
countyVote = as.factor(ifelse(countyVote == "Yes", 1, 0)),
majorityRace = as.factor(case_when(
majorityRace == "White" ~ 1,
majorityRace == "Black" ~ 2,
majorityRace == "Latino" ~ 3,
majorityRace == "Asian" ~ 4,
majorityRace == "American Indian" ~ 5)),
medianIncome = as.numeric(medianIncome),
medianIncome = log(medianIncome),
zip = as.factor(zip)
)
joinedDataSet <- left_join(fullDataSet, demographics) %>%
fill(c(6:15), .direction = c("down"))
str(joinedDataSet)
dim(joinedDataSet)
#[1] 2937   15
#split data into training and test
#using simple random sampling using caret package
set.seed(123)  # for reproducibility
index_2 <- createDataPartition(joinedDataSet$topic, p = 0.7,
list = FALSE)
train <- joinedDataSet[index_2, ]
test  <- joinedDataSet[-index_2, ]
### CODE BLOCK: Following code for Random Forest model ###
## first default random forest model using the caret package
random_forest <- train(topic ~ .,
data = train,
method = "ranger")
random_forest
# your filepath here
path <- "/Users/madelinebrady/Documents/GitHub/Hertie-ML-TADA-Project/final"
# adding independent variables to data set
fullDataSet <- read.csv("preppedDataSet.csv")
demographics <- read.csv("demoData.csv")
setwd("~/Documents/GitHub/Hertie-ML-TADA-Project/final")
library(caret)
library(dplyr)    # for data wrangling
library(ggplot2)  # for awesome graphics
library(rsample)
library(ranger)   # for a fast c++ implementation of random forest
library(h2o)
library(dplyr)
library(tidyverse)
library(randomForest)
# h2o set-up
h2o.no_progress()  # turn off h2o progress bars
h2o.init()         # launch h2o
# your filepath here
path <- "/Users/madelinebrady/Documents/GitHub/Hertie-ML-TADA-Project/final"
# adding independent variables to data set
fullDataSet <- read.csv("preppedDataSet.csv")
demographics <- read.csv("demoData.csv")
fullDataSet <- fullDataSet %>%
mutate(date = as.Date(date),
topic = as.factor(topic))
demographics <- demographics %>%
mutate(date = as.Date(date),
children = as.factor(ifelse(children == "Yes", 1, 0)),
murderSuicide = as.factor(ifelse(murderSuicide == "Yes", 1, 0)),
countyVote = as.factor(ifelse(countyVote == "Yes", 1, 0)),
majorityRace = as.factor(case_when(
majorityRace == "White" ~ 1,
majorityRace == "Black" ~ 2,
majorityRace == "Latino" ~ 3,
majorityRace == "Asian" ~ 4,
majorityRace == "American Indian" ~ 5)),
medianIncome = as.numeric(medianIncome),
medianIncome = log(medianIncome),
zip = as.factor(zip)
)
joinedDataSet <- left_join(fullDataSet, demographics) %>%
fill(c(6:15), .direction = c("down"))
str(joinedDataSet)
dim(joinedDataSet)
#[1] 2937   15
#split data into training and test
#using simple random sampling using caret package
set.seed(123)  # for reproducibility
index_2 <- createDataPartition(joinedDataSet$topic, p = 0.7,
list = FALSE)
train <- joinedDataSet[index_2, ]
test  <- joinedDataSet[-index_2, ]
### CODE BLOCK: Following code for Random Forest model ###
## first default random forest model using the caret package
random_forest <- train(topic ~ .,
data = train,
method = "ranger")
