tuneLength = 10
)
?ksvm
ksvmNews <- ksvm(topic ~ ., data = trainDataNews, type = "kbb-svc", kernel = "rbfdot",
C = 0.5, prob.model = TRUE)
# predict probabilities using the vip package
prob_yes <- function(ksvmNews, newdata) {
predict(ksvmNews, newdata = trainDataNews, type = "prob")[, "Yes"]
}
prob_yes
prob_yes
couple(ksvmNews, coupler = "minpair")
newsProbs <- predict(ksvmNews, trainDataNews, type = "probabilities", coupler = "minpair")
ksvmNews <- ksvm(topic ~ ., data = trainDataNews, type = "kbb-svc", kernel = "rbfdot",
C = 0.5, prob.model = TRUE)
newsProbs <- predict(ksvmNews, trainDataNews, type = "probabilities", coupler = "minpair")
predict.ksvm
?predict.ksvm
?ksvm
ksvmNews <- ksvm(topic ~ ., data = trainDataNews, type = "kbb-svc", kernel = "rbfdot", C = 0.5, prob.model = TRUE)
ksvmNews
newsPredict <- predict(ksvmNews, trainDataNews, type = "probabilities", coupler = "minpair")
couple(ksvmNews, coupler = "minpair")
?couple
couple(probin, coupler = "minpair")
couple(probin = ksvmNews, coupler = "minpair")
library(caret)
library(h2o)
library(tidyverse)
h2o.init()
set.seed(123)
joinedDataSet <- read.csv("joinedDataSet.csv")
joinedDataSet$topic <- as.factor(joinedDataSet$topic)
# downsampling the data
balancedData <- downSample(x = joinedDataSet[, -1],
y = joinedDataSet$topic)
# split data into training and test
index <- createDataPartition(balancedData$topic, p = 0.7,
list = FALSE)
trainData <- balancedData[index, ]
testData  <- balancedData[-index, ]
# convert data to h2o objects
trainH2o <- as.h2o(trainData)
testH2o <- as.h2o(testData)
# specifying variables
response <- "topic"
predictors <- trainData %>%
select(-document, -gamma, -location, -Class, -topic) %>%
colnames()
nFeatures <- length(predictors)
### RANDOM FOREST ###
# OOB model
h2oRF1 <- h2o.randomForest(
x = predictors,
y = response,
training_frame = trainH2o,
ntrees = 1000,
max_depth = 30,
min_rows = 1,
sample_rate = 0.8,
nfolds = 10,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
seed = 123,
stopping_rounds = 50,
stopping_metric = "misclassification",
stopping_tolerance = 0
)
# hyperparameter tuning
hyperGridRF <- list(
mtries = c(-1, 1, 2, 3),
min_rows = c(1, 3, 5, 10),
max_depth = c(10, 20, 30),
sample_rate = c(.55, .632, .70, .80)
)
searchCriteriaRF <- list(
strategy = "RandomDiscrete",
stopping_metric = "misclassification",
stopping_tolerance = 0.001,   # stop if we don't experience 0.1% improvement
stopping_rounds = 10,         # over the last 10 models
max_runtime_secs = 60*10      # or cut grid search off at 10 minutes
)
randomGridRF <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_random_grid",
x = predictors,
y = response,
training_frame = trainH2o,
hyper_params = hyperGrid,
ntrees = nFeatures * 10,
seed = 123,
stopping_metric = "misclassification",
stopping_rounds = 10,           # stop adding trees if we don't experience
stopping_tolerance = 0.005,     # 0.05 improvement in error over last 10 trees
search_criteria = searchCriteria
)
gridPerformanceRF <- h2o.getGrid(
grid_id = "rf_random_grid",
sort_by = "mean_per_class_error",
decreasing = FALSE
)
### GBM ###
h2oGBM <- h2o.gbm(
x = predictors,
y = response,
training_frame = trainH2o,
ntrees = 500,
learn_rate = 0.01,
max_depth = 7,
min_rows = 5,
sample_rate = 0.8,
nfolds = 10,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
seed = 123,
stopping_rounds = 50,
stopping_metric = "misclassification",
stopping_tolerance = 0
)
# hyperparameter tuning
hyperGridGBM <- list(
sample_rate = c(0.5, 0.75, 1),              # row subsampling
col_sample_rate = c(0.5, 0.75, 1),          # col subsampling for each split
col_sample_rate_per_tree = c(0.5, 0.75, 1)  # col subsampling for each tree
)
searchCriteriaGBM <- list(
strategy = "RandomDiscrete",
stopping_metric = "misclassification",
stopping_tolerance = 0.001,
stopping_rounds = 10,
max_runtime_secs = 60*60
)
gridGBM <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid",
x = predictors,
y = response,
training_frame = trainH2o,
hyper_params = hyperGrid,
ntrees = 6000,
learn_rate = 0.01,
max_depth = 7,
min_rows = 5,
nfolds = 10,
stopping_rounds = 10,
stopping_tolerance = 0,
search_criteria = searchCriteria,
seed = 123
)
gridPerformanceGBM <- h2o.getGrid(
grid_id = "gbm_grid",
sort_by = "mean_per_class_error",
decreasing = FALSE
)
gridPerformanceGBM
### STACKING MODELS ###
# model
ensembleTree <- h2o.stackedEnsemble(
x = predictors, y = response, training_frame = trainH2o, model_id = "example_2",
base_models = list(h2oRF1, h2oGBM),
metalearner_algorithm = "drf"
)
# evaluation
getMisclass <- function(model) {
results <- h2o.performance(model, newdata = testH2o)
results@metrics$mean_per_class_error
}
list(h2oRF1, h2oGBM) %>%
purrr::map_dbl(getMisclass)
h2o.performance(ensembleTree, newdata = testH2o)@metrics$mean_per_class_error
### SVM ###
library(e1071)
X <- trainData %>%
select(medianIncome, total, zip, dead, injured, childrenHarmed, majorityRace, countyVote)
Y <- trainData$topic
model <- svm(x = X, y = Y, probability = TRUE)
pred_prob <- predict(model, x = X, decision.values = TRUE, probability = TRUE)
setwd("~/Documents/GitHub/Hertie-ML-TADA-Project/data")
library(caret)
library(h2o)
library(tidyverse)
h2o.init()
set.seed(123)
joinedDataSet <- read.csv("joinedDataSet.csv")
joinedDataSet$topic <- as.factor(joinedDataSet$topic)
# downsampling the data
balancedData <- downSample(x = joinedDataSet[, -1],
y = joinedDataSet$topic)
# split data into training and test
index <- createDataPartition(balancedData$topic, p = 0.7,
list = FALSE)
trainData <- balancedData[index, ]
testData  <- balancedData[-index, ]
# convert data to h2o objects
trainH2o <- as.h2o(trainData)
testH2o <- as.h2o(testData)
# specifying variables
response <- "topic"
predictors <- trainData %>%
select(-document, -gamma, -location, -Class, -topic) %>%
colnames()
nFeatures <- length(predictors)
### RANDOM FOREST ###
# OOB model
h2oRF1 <- h2o.randomForest(
x = predictors,
y = response,
training_frame = trainH2o,
ntrees = 1000,
max_depth = 30,
min_rows = 1,
sample_rate = 0.8,
nfolds = 10,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
seed = 123,
stopping_rounds = 50,
stopping_metric = "misclassification",
stopping_tolerance = 0
)
# hyperparameter tuning
hyperGridRF <- list(
mtries = c(-1, 1, 2, 3),
min_rows = c(1, 3, 5, 10),
max_depth = c(10, 20, 30),
sample_rate = c(.55, .632, .70, .80)
)
searchCriteriaRF <- list(
strategy = "RandomDiscrete",
stopping_metric = "misclassification",
stopping_tolerance = 0.001,   # stop if we don't experience 0.1% improvement
stopping_rounds = 10,         # over the last 10 models
max_runtime_secs = 60*10      # or cut grid search off at 10 minutes
)
randomGridRF <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_random_grid",
x = predictors,
y = response,
training_frame = trainH2o,
hyper_params = hyperGrid,
ntrees = nFeatures * 10,
seed = 123,
stopping_metric = "misclassification",
stopping_rounds = 10,           # stop adding trees if we don't experience
stopping_tolerance = 0.005,     # 0.05 improvement in error over last 10 trees
search_criteria = searchCriteria
)
gridPerformanceRF <- h2o.getGrid(
grid_id = "rf_random_grid",
sort_by = "mean_per_class_error",
decreasing = FALSE
)
### GBM ###
h2oGBM <- h2o.gbm(
x = predictors,
y = response,
training_frame = trainH2o,
ntrees = 500,
learn_rate = 0.01,
max_depth = 7,
min_rows = 5,
sample_rate = 0.8,
nfolds = 10,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
seed = 123,
stopping_rounds = 50,
stopping_metric = "misclassification",
stopping_tolerance = 0
)
# hyperparameter tuning
hyperGridGBM <- list(
sample_rate = c(0.5, 0.75, 1),              # row subsampling
col_sample_rate = c(0.5, 0.75, 1),          # col subsampling for each split
col_sample_rate_per_tree = c(0.5, 0.75, 1)  # col subsampling for each tree
)
searchCriteriaGBM <- list(
strategy = "RandomDiscrete",
stopping_metric = "misclassification",
stopping_tolerance = 0.001,
stopping_rounds = 10,
max_runtime_secs = 60*60
)
gridGBM <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid",
x = predictors,
y = response,
training_frame = trainH2o,
hyper_params = hyperGrid,
ntrees = 6000,
learn_rate = 0.01,
max_depth = 7,
min_rows = 5,
nfolds = 10,
stopping_rounds = 10,
stopping_tolerance = 0,
search_criteria = searchCriteria,
seed = 123
)
gridPerformanceGBM <- h2o.getGrid(
grid_id = "gbm_grid",
sort_by = "mean_per_class_error",
decreasing = FALSE
)
gridPerformanceGBM
### STACKING MODELS ###
# model
ensembleTree <- h2o.stackedEnsemble(
x = predictors, y = response, training_frame = trainH2o, model_id = "example_2",
base_models = list(h2oRF1, h2oGBM),
metalearner_algorithm = "drf"
)
# evaluation
getMisclass <- function(model) {
results <- h2o.performance(model, newdata = testH2o)
results@metrics$mean_per_class_error
}
list(h2oRF1, h2oGBM) %>%
purrr::map_dbl(getMisclass)
h2o.performance(ensembleTree, newdata = testH2o)@metrics$mean_per_class_error
### SVM ###
library(e1071)
X <- trainData %>%
select(medianIncome, total, zip, dead, injured, childrenHarmed, majorityRace, countyVote)
Y <- trainData$topic
model <- svm(x = X, y = Y, probability = TRUE)
pred_prob <- predict(model, x = X, decision.values = TRUE, probability = TRUE)
setwd("~/Documents/GitHub/Hertie-ML-TADA-Project/data")
library(caret)
library(h2o)
library(tidyverse)
library(vip)
h2o.init()
set.seed(123)
joinedDataSet <- read.csv("joinedDataSet.csv")
joinedDataSet$topic <- as.factor(joinedDataSet$topic)
View(joinedDataSet)
## overall top topic by outlet (before downsampling)
## calculate topic proportion for each outlet
breitbartData %>% filter(outlet = breitbart)
## overall top topic by outlet (before downsampling)
## calculate topic proportion for each outlet
breitbartData <- joinedDataSet %>% filter(outlet = breitbart)
## overall top topic by outlet (before downsampling)
## calculate topic proportion for each outlet
breitbartData <- joinedDataSet %>% filter(outlet == breitbart)
## overall top topic by outlet (before downsampling)
## calculate topic proportion for each outlet
breitbartData <- joinedDataSet %>% filter(outlet == "breitbart")
## calculate topic proportion for each outlet
breitbartData <- joinedDataSet %>% filter(outlet == "breitbart")
foxData <- joinedDataSet %>% filter(outlet == "foxnews")
nytData <- joinedDataSet %>% filter(outlet == "nytimes")
wsjData <- joinedDataSet %>% filter(outlet == "wallstreetjournal")
thingprogData <- joinedDataSet %>% filter(outlet == "thinkprogress")
View(foxData)
count(distinct(breitbartData$outlet))
distinct(breitbartData$outlet)
ggplot(breitbartData, aes(x=factor(topic)))+
geom_bar(stat="bin", width=0.7, fill="steelblue")+
theme_minimal()
?count()
nrow(breitbartData$topic)
length(breitbartData$topic)
length(breitbartData$topic == 1)
length(breitbartData$topic = 1)
distinct(breitbartData$topic)
df <- data.frame(val=c(1,2,3,4,5,6,7,4,5,6,2,2,2,2,2,1,1,1,25,28,25,29,3));
table(df$val)
table(breitbartData$topic)
breitbartCount <- table(breitbartData$topic)
breitbartCount[1,]
[breitbartCount$1,]
breitbartCount <- as.data.frame(table(breitbartData$topic))
?which
which(breitbartCount$Var1)
breitbartProp <-  which(breitbartCount$Var1), / length(breitbartData$topic)
M <- freq(breitbartData$outlet, digits = 1, cum = TRUE, total = TRUE, exclude = NULL,
sort = "", valid = TRUE, na.last = TRUE)
?count
count(breitbartCount, sort = TRUE)
count(breitbartData$topic, sort = TRUE)
breitbartCount
breitbartProp <- breitbartCount[1,1] length(breitbartData$topic)
breitbartCount[1,1]
breitbartCount[1,2]
breitbartData <- joinedDataSet %>% filter(outlet == "breitbart")
breitbartCount <- as.data.frame(table(breitbartData$topic))
breitbartProp <- breitbartCount[1,2] / length(breitbartData$topic)
?table
max(breitbartCount$Freq)
breitbartData <- joinedDataSet %>% filter(outlet == "breitbart")
breitbartCount <- as.data.frame(table(breitbartData$topic))
breitbartProp <- max(breitbartCount$Freq) / length(breitbartData$topic)
nytData <- joinedDataSet %>% filter(outlet == "nytimes")
foxData <- joinedDataSet %>% filter(outlet == "foxnews")
foxCount <- as.data.frame(table(foxData$topic))
foxProp <- max(foxCount$Freq) / length(foxData$topic)
nytData <- joinedDataSet %>% filter(outlet == "nytimes")
nytCount <- as.data.frame(table(nytData$topic))
nytProp <- max(nytCount$Freq) / length(nytData$topic)
wsjData <- joinedDataSet %>% filter(outlet == "wallstreetjournal")
wsjCount <- as.data.frame(table(wsjData$topic))
wsjProp <- max(wsjCount$Freq) / length(wsjData$topic)
thingprogData <- joinedDataSet %>% filter(outlet == "thinkprogress")
thingprogCount <- as.data.frame(table(thingprogData$topic))
thingprogProp <- max(thingprogCount$Freq) / length(thingprogData$topic)
library(tidyverse)
joinedDataSet <- read.csv("joinedDataSet.csv")
joinedDataSet$topic <- as.factor(joinedDataSet$topic)
# vizualizations
## overall top topic by outlet (before downsampling)
## calculate topic proportion for each outlet
breitbartData <- joinedDataSet %>% filter(outlet == "breitbart")
breitbartCount <- as.data.frame(table(breitbartData$topic))
breitbartProp <- max(breitbartCount$Freq) / length(breitbartData$topic)
foxData <- joinedDataSet %>% filter(outlet == "foxnews")
foxCount <- as.data.frame(table(foxData$topic))
foxProp <- max(foxCount$Freq) / length(foxData$topic)
nytData <- joinedDataSet %>% filter(outlet == "nytimes")
nytCount <- as.data.frame(table(nytData$topic))
nytProp <- max(nytCount$Freq) / length(nytData$topic)
wsjData <- joinedDataSet %>% filter(outlet == "wallstreetjournal")
wsjCount <- as.data.frame(table(wsjData$topic))
wsjProp <- max(wsjCount$Freq) / length(wsjData$topic)
thinkprogData <- joinedDataSet %>% filter(outlet == "thinkprogress")
thinkprogCount <- as.data.frame(table(thinkprogData$topic))
thinkprogProp <- max(thinkprogCount$Freq) / length(thinkprogData$topic)
overallTopTopic <- rbind(outlet_names, topic_proportions)
outlet_names <- ("breitbart", "foxnews", "nytimes", "wallstreetjournal", "thinkprogress")
topic_proportions <- (breitbartProp, foxProp, nytProp, wsjProp, thinkprogProp)
overallTopTopic <- rbind(outlet_names, topic_proportions)
outlet_names <- ("breitbart", "foxnews", "nytimes", "wallstreetjournal", "thinkprogress")
overallTopTopic <- data.frame("breitbart" = breitbartProp, "foxnews" = foxProp)
View(overallTopTopic)
outlet_names <- c("breitbart", "foxnews", "nytimes", "wallstreetjournal", "thinkprogress")
topic_proportions <- c(breitbartProp, foxProp, nytProp, wsjProp, thinkprogProp)
overallTopTopic <- rbind(outlet_names, topic_proportions)
View(overallTopTopic)
overallTopTopic <- cbind(outlet_names, topic_proportions)
View(breitbartCount)
(table(foxData$topic)
table(foxData$topic)
View(foxCount)
foxCount
nytCount
wsjCount
thinkprogCount
?mutate
breitbartCount %>% mutate(n = length(breitbartData$topic)
topic_proportion = "Freq"/"n"
)
breitbartCount %>% mutate(n = length(breitbartData$topic),
topic_proportion = "Freq"/"n"
)
breitbartCount %>% mutate(n = length(breitbartData$topic),
topic_proportion = Freq / n
)
breitbartProp <- max(breitbartCount$topic_proportion)
breitbartProp <- max(breitbartCount$topic_proportion)
breitbartProp <- max(breitbartCount$topic_proportion) / length(breitbartData$topic)
breitbartProp <- max(breitbartCount$Freq) / length(breitbartData$topic)
breitbartProp
breitbartCount %>% mutate(n = length(breitbartData$topic),
topic_proportion = Freq / n
)
breitbartTopTop <- [max(breitbartCount$Freq), breitbartCount$Var1]
ggplot(breitbartCount, aes(x=factor(topic_proportions)))+
geom_bar(stat="bin", width=0.7, fill="steelblue")+
theme_minimal()
ggplot(breitbartCount, aes(x=topic_proportions)+
geom_bar(stat="bin", width=0.7, fill="steelblue")+
theme_minimal()
ggplot(breitbartCount, aes(x=topic_proportions)+
geom_bar(stat="bin", width=0.7, fill="steelblue")+
theme_minimal()
geom_bar(stat="bin", width=0.7, fill="steelblue")
ggplot(breitbartCount, aes(x=topic_proportions)+
geom_bar(stat="bin", width=0.7, fill="steelblue")
ggplot(breitbartData, aes(x=factor(topic)))+
geom_bar(stat="bin", width=0.7, fill="steelblue")+
theme_minimal()
# Don't map a variable to y
ggplot(breitbartData, aes(x=factor(topic)))+
geom_bar(stat="bin", width=0.7, fill="steelblue")
# Don't map a variable to y
ggplot(breitbartData, aes(x=factor(topic)))+
geom_bar(stat="count", width=0.7, fill="steelblue")
install.packages("wesanderson")
library(wesanderson)
?ggplot
?geom_bar
?fill
geom_bar(stat="count", width=0.7, scale_fill_manual(values=wes_palette(n=3, name="GrandBudapest"))
ggplot(breitbartData, aes(x=factor(topic)))+
geom_bar(stat="count", width=0.7, scale_fill_manual(values=wes_palette(n=3, name="GrandBudapest"))
geom_bar(stat="count", width=0.7, scale_fill_manual(values=wes_palette(n=3, name="GrandBudapest"))
geom_bar(stat="count", width=0.7, scale_fill_manual(values=wes_palette(n=6, name="GrandBudapest"))
ggplot(breitbartData, aes(x=factor(topic)))+
geom_bar(stat="count", width=0.7, scale_fill_manual(values=wes_palette(n=6, name="GrandBudapest"))
ggplot(breitbartData, aes(x=factor(topic)))+
geom_bar(stat="count", width=0.7) +scale_fill_manual(values=wes_palette(n=3, name="GrandBudapest"))
ggplot(breitbartData, aes(x=factor(topic)))+
geom_bar(stat="count", width=0.7) +scale_fill_manual(values=wes_palette(n=6, name="GrandBudapest"))
library(wesanderson)
ggplot(breitbartData, aes(x=factor(topic)))+
geom_bar(stat="count", width=0.7) +scale_fill_manual(values=wes_palette(n=6, name="GrandBudapest"))
geom_bar(stat="count", width=0.7, scale_fill_manual(values=wes_palette(n=6, name="GrandBudapest"))
?scale_fill_manual
?scale_fill_manual
